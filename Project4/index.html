<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Diffusion Models</title>
    <style>
        body {
            font-family: Georgia, serif;
            max-width: 800px;
            margin: 40px auto;
            padding: 20px;
            line-height: 1.6;
            color: #000;
            background: #fff;
        }
        h1 {
            text-align: center;
            font-size: 28px;
            margin-bottom: 10px;
        }
        h2 {
            font-size: 22px;
            margin-top: 30px;
            border-bottom: 2px solid #333;
        }
        h3 {
            font-size: 18px;
            margin-top: 20px;
        }
        .header {
            text-align: center;
            margin-bottom: 40px;
        }
        .figure {
            margin: 30px 0;
            text-align: center;
        }
        .figure img {
            max-width: 100%;
            height: auto;
        }
        .figure-caption {
            margin-top: 10px;
            font-style: italic;
            font-size: 14px;
        }
        p {
            margin-bottom: 12px;
            text-align: justify;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Project 4: Diffusion Models</h1>
        <p><strong>TJ DiMeola</strong><br>
        Computer Vision CSCI 581<br>
        Instructor: Dr. Hawk Wang<br>
        December 9, 2025</p>
    </div>

    <h2>Part 0: Setup</h2>
    <p>The DeepFloyd IF diffusion model is used in this project. DeepFloyd has been trained and released by Stability AI. Its first stage generates images with size 64×64, the second stage takes the outputs of the first stage and produces images with size 256×256.</p>

    <p>DeepFloyd is a text-to-image model, which takes text prompts as input and outputs images based on the input text. In this project, the prompt "a high quality photo" is often used, which acts as a null prompt for unconditional generation.</p>

    <h3>Random Seed</h3>
    <p>Random seed = 180 (used throughout all experiments)</p>

    <h2>Part 1: Understanding the Forward & Reverse Processes</h2>

    <h3>1.1 Forward Process (Adding Noise)</h3>
    <p>The forward process gradually adds Gaussian noise to a clean image according to a predefined noise schedule. The implementation adds noise at timesteps t = 250, 500, and 750.</p>

    <div class="figure">
        <img src="images/noisyImageAtTime_250.png" alt="Noisy at t=250">
        <img src="images/noisyImageAtTime_500.png" alt="Noisy at t=500">
        <img src="images/noisyImageAtTime_750.png" alt="Noisy at t=750">
        <p class="figure-caption">Figure 1: Noisy images at t = 250, 500, 750</p>
    </div>

    <h3>1.2 Traditional Denoising (Gaussian Blur)</h3>
    <p>Gaussian filtering is applied to denoise the noisy images. While Gaussian blur can reduce noise, it also removes important image details and cannot selectively preserve edges.</p>

    <h3>1.3 One-Step Denoising (Using Pretrained UNet)</h3>
    <p>Using a pretrained diffusion model UNet, we can predict noise and perform one-step denoising. Results show that one-step denoising works well for low noise levels (t=250) but struggles at higher noise levels (t=750).</p>

    <div class="figure">
        <img src="images/denoisedImage_250.png" alt="Denoised t=250">
        <img src="images/denoisedImage_500.png" alt="Denoised t=500">
        <img src="images/denoisedImage_750.png" alt="Denoised t=750">
        <p class="figure-caption">Figure 2: One-step denoising results</p>
    </div>

    <h3>1.4 Iterative Denoising</h3>
    <p>Iterative denoising with strided timesteps (stride=30) significantly improves results. The process gradually removes noise over multiple steps, producing clean, recognizable images.</p>

    <div class="figure">
        <img src="images/iterative_denoising.gif" alt="Iterative denoising animation">
        <p class="figure-caption">Figure 3: Iterative denoising process (animated)</p>
    </div>

    <h2>Part 2: Diffusion Model Sampling</h2>

    <h3>2.1 Generating Images from Pure Noise</h3>
    <p>Starting from pure random noise, the diffusion model can generate realistic images through the iterative denoising process.</p>

    <div class="figure">
        <img src="images/part2_0_sample_1.png" alt="Sample 1" style="width:18%">
        <img src="images/part2_0_sample_2.png" alt="Sample 2" style="width:18%">
        <img src="images/part2_0_sample_3.png" alt="Sample 3" style="width:18%">
        <img src="images/part2_0_sample_4.png" alt="Sample 4" style="width:18%">
        <img src="images/part2_0_sample_5.png" alt="Sample 5" style="width:18%">
        <p class="figure-caption">Figure 4: Five generated images from pure noise</p>
    </div>

    <h3>2.2 Classifier-Free Guidance (CFG)</h3>
    <p>CFG improves image quality and prompt adherence by combining conditional and unconditional noise estimates with guidance scale γ=7.</p>

    <div class="figure">
        <img src="images/part2_2_cfg_sample_1.png" alt="CFG 1" style="width:18%">
        <img src="images/part2_2_cfg_sample_2.png" alt="CFG 2" style="width:18%">
        <img src="images/part2_2_cfg_sample_3.png" alt="CFG 3" style="width:18%">
        <img src="images/part2_2_cfg_sample_4.png" alt="CFG 4" style="width:18%">
        <img src="images/part2_2_cfg_sample_5.png" alt="CFG 5" style="width:18%">
        <p class="figure-caption">Figure 5: Images generated with CFG (γ=7)</p>
    </div>

    <h3>2.3 Image-to-Image Translation (SDEdit)</h3>
    <p>By adding noise to a real image and then denoising, we can create variations. Higher noise levels (i_start=20) produce more creative edits, while lower noise levels (i_start=1) stay closer to the original.</p>

    <div class="figure">
        <img src="images/original_20251030_194251.png" alt="Original 1" style="width:30%">
        <img src="images/edit_20251030_194251_i5_t840.png" alt="Edit 1" style="width:30%">
        <img src="images/edit_20251030_194251_i20_t390.png" alt="Edit 2" style="width:30%">
        <p class="figure-caption">Figure 6: Original image and edits at different noise levels</p>
    </div>

    <h2>Part 3: Visual Anagrams</h2>
    <p>Visual anagrams create images that reveal different subjects when flipped. By averaging noise estimates from normal and flipped orientations, we create optical illusions.</p>

    <div class="figure">
        <img src="images/illusion_normal.png" alt="Normal" style="width:45%">
        <img src="images/illusion_flipped.png" alt="Flipped" style="width:45%">
        <p class="figure-caption">Figure 7: Visual anagram - "a lithograph of a skull" (left) and "a lithograph of waterfalls" (flipped, right)</p>
    </div>

    <h2>Part 4: Training Diffusion Models from Scratch</h2>

    <h3>4.1 Unconditional Denoising UNet</h3>
    <p>Trained a UNet to denoise MNIST digits with σ=0.5 noise. The denoising task converges quickly (20 epochs) as it only needs to remove fixed-level noise.</p>

    <div class="figure">
        <img src="images/figure3_noising_process.png" alt="Noising" style="width:100%">
        <p class="figure-caption">Figure 8: Noising process at different σ values</p>
    </div>

    <div class="figure">
        <img src="images/figure4_loss_curve.png" alt="Loss" style="width:60%">
        <p class="figure-caption">Figure 9: Training loss curve</p>
    </div>

    <h3>4.2 Time-Conditioned DDPM</h3>
    <p>Full diffusion model that generates MNIST digits from pure noise using time-step conditioning with 300 timesteps.</p>

    <div class="figure">
        <img src="images/figure10_loss_curve.png" alt="Time Loss" style="width:60%">
        <p class="figure-caption">Figure 10: Time-conditioned DDPM training loss</p>
    </div>

    <div class="figure">
        <img src="images/samples_epoch_5.png" alt="Epoch 5" style="width:45%">
        <img src="images/samples_epoch_20.png" alt="Epoch 20" style="width:45%">
        <p class="figure-caption">Figure 11: Generated samples at epochs 5 (left) and 20 (right)</p>
    </div>

    <h3>4.3 Class-Conditioned DDPM</h3>
    <p>Generates specific digits (0-9) on demand using class conditioning and classifier-free guidance (10% unconditioning, γ=5.0).</p>

    <h3>Key Finding</h3>
    <p><strong>Denoising vs Generation:</strong> Simple denoising (Part 4.1) converges in 20 epochs, but generation from pure noise (Parts 4.2-4.3) requires 100-500+ epochs for high-quality results. At 20 epochs, loss decreases but sample quality appears degraded - this early learning paradox indicates the model needs extended training.</p>

    <hr style="margin: 40px 0">
    <p style="text-align: center; color: #666">
        <strong>Computer Vision CSCI 581</strong><br>
        University of Mississippi | Fall 2025<br>
        <a href="tjdimeola_Project4.pdf">Download PDF Report</a>
    </p>
</body>
</html>
