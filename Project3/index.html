<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 3: Object Detection and HOI Analysis - TJ DiMeola</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            color: #333;
        }

        .container {
            background-color: white;
            padding: 40px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }

        h1 {
            text-align: center;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
        }

        h2 {
            color: #2980b9;
            margin-top: 40px;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 8px;
        }

        h3 {
            color: #34495e;
            margin-top: 25px;
        }

        .header-info {
            text-align: center;
            margin-bottom: 40px;
            color: #7f8c8d;
        }

        .header-info p {
            margin: 5px 0;
        }

        .highlight-box {
            background-color: #ecf0f1;
            border-left: 4px solid #3498db;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 4px;
        }

        .code-box {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
            white-space: pre-wrap;
            margin: 20px 0;
            overflow-x: auto;
        }

        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .image-container {
            text-align: center;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .image-caption {
            margin-top: 8px;
            font-style: italic;
            color: #7f8c8d;
            font-size: 14px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 14px;
        }

        table th {
            background-color: #34495e;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: bold;
        }

        table td {
            padding: 10px;
            border: 1px solid #ddd;
        }

        table tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        table tr:hover {
            background-color: #ecf0f1;
        }

        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        li {
            margin: 8px 0;
        }

        .metric-summary {
            background-color: #e8f4f8;
            border: 1px solid #3498db;
            border-radius: 4px;
            padding: 15px;
            margin: 20px 0;
        }

        .metric-summary h4 {
            color: #2980b9;
            margin-top: 0;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px;
            }

            .image-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Report 3</h1>
        <h1 style="border: none; margin-top: -20px;">Project 3: Object Detection and Human–Object Interaction Analysis</h1>

        <div class="header-info">
            <p><strong>TJ DiMeola</strong></p>
            <p>Course: Computer Vision CSCI 581</p>
            <p>Instructor: Dr. Hawk Wang</p>
            <p>Date: November 16, 2025</p>
        </div>

        <h2>Project Objective</h2>
        <p>The goal of this project is to deepen understanding of object detection pipelines and extend that knowledge to higher-level visual reasoning.</p>

        <h2>Part 1: Lightweight Object Detection</h2>
        <p>Implement and train a lightweight detection model. Here Faster R-CNN with MobileNet is applied to two banana detection datasets:</p>
        <ul>
            <li>banana-detection (d2l)</li>
            <li>banana_classification (Kaggle)</li>
        </ul>

        <div class="highlight-box">
            <p>Since the Kaggle banana dataset (<a href="https://www.kaggle.com/datasets/atrithakar/banana-classification" target="_blank">https://www.kaggle.com/datasets/atrithakar/banana-classification</a>) had no labels (bounding box data in csv form), I used qwen to generate bounding boxes for the data and then calculated the bbox values to match the csv format of the d2l dataset.</p>

            <p><strong>This VLM-as-annotator approach generated 921 accurate labels allowing me to train on a much larger mixed-dataset leading to greatly improved in-the-wild image detection (from 1/7 to 6/7 success rate). This effectively demonstrates that VLMs can serve as effective zero-shot data labelers for training traditional detectors.</strong></p>
        </div>

        <h3>Tasks and Deliverables:</h3>

        <h4>Plot training loss curves (class error and bounding box error)</h4>
        <p>Below there are two TL curves. The first based on banana-prediction (d2l) alone, the second based on both the d2l and the Kaggle datasets.</p>

        <div class="image-container" style="margin: 20px 0;">
            <img src="d2l_training_losses.png" alt="D2L Training Losses" style="max-width: 90%;">
            <p class="image-caption">Training losses for D2L dataset only</p>
        </div>
        <div class="image-container" style="margin: 20px 0;">
            <img src="training_losses_mixed.png" alt="Mixed Training Losses" style="max-width: 90%;">
            <p class="image-caption">Training losses for mixed dataset (D2L + Kaggle)</p>
        </div>

        <h4>Show 5 sample detections</h4>
        <div class="image-grid">
            <div class="image-container">
                <img src="images/ban1.png" alt="Detection Sample 1">
            </div>
            <div class="image-container">
                <img src="images/ban2.png" alt="Detection Sample 2">
            </div>
            <div class="image-container">
                <img src="images/ban3.jpg" alt="Detection Sample 3">
            </div>
            <div class="image-container">
                <img src="images/ban4.jpg" alt="Detection Sample 4">
            </div>
            <div class="image-container">
                <img src="images/ban5.jpg" alt="Detection Sample 5">
            </div>
        </div>

        <h4>Test the detection model with random (in-the-wild) banana images</h4>

        <p><strong>Were there any failure cases?</strong></p>
        <p>The d2l val set had no failure cases, on the Kaggle set, failed in all cases. The model generated by mixed training (d2l + Kaggle) failed on only the last image (the banana on the grass and gravel).</p>

        <p><strong>Discuss the possible reasons that caused the detection to fail:</strong></p>
        <p>The d2l case failed on all "real" bananas because it was really just a single banana. The transforms were poor (they could at least elongated it, reducing the arc in several instances). The mixed case only failed on the image #7 because the background was simply too complex for Faster R-CNN to really parse.</p>

        <h2>Part 2: Non-Maximum Suppression (NMS)</h2>

        <h3>Tasks and Deliverables</h3>

        <h4>Visualize outputs before/after NMS</h4>
        <div class="image-container" style="margin: 20px 0;">
            <img src="images/nmsALL.jpg" alt="NMS Comparison" style="max-width: 100%;">
            <p class="image-caption">NMS comparison showing all three modes</p>
        </div>

        <h4>Compare with PyTorch's NMS implementation. Any difference?</h4>
        <p>I did not find any differences when these two implementations were run on the same test (zero-shot) images.</p>

        <h4>Discuss its purpose and limitations:</h4>
        <p>NMS eliminates redundant overlapping bounding boxes by keeping only the highest-confidence detection and suppressing nearby boxes above a specified IoU threshold. This prevents multiple detections of the same object (see the first line of the image above, where no nms was provided).</p>

        <p>Such fixed IoU thresholds fail in crowded scenes because potentially valid detections can be suppressed when objects genuinely overlap. NMS is a greedy algorithm that can't adapt to varying object densities and may discard correct boxes if a slightly higher-confidence but incorrect box overlaps them.</p>

        <h2>Part 3: Human–Object Interaction (HOI) Analysis using VLMs</h2>
        <p>Perform zero-shot HOI analysis using VLMs on a subset of HICO-DET dataset.</p>

        <h3>Tasks and Deliverables</h3>

        <h4>Use one (or more) open-source or closed-source VLMs to predict human–object interactions</h4>
        <p>Here I used <strong>Claude Sonnet 4.5</strong> and <strong>Qwen VL Max</strong>.</p>

        <h4>Come up with your prompt to guide the VLMs to predict &lt;interaction object&gt;</h4>
        <p>For example, &lt;hold apple&gt;, &lt;ride bicycle&gt;. Prompt used:</p>

        <div class="code-box">Analyze this image and identify all human-object interactions.

For each interaction, provide the answer in this exact format:
&lt;verb object&gt;

Where:
- verb = the action being performed (e.g., riding, eating, holding, sitting on)
- object = the object being interacted with (e.g., bicycle, pizza, umbrella, chair)

Rules:
1. Only report interactions you can CLEARLY see in the image
2. Use simple, concrete verbs (not complex phrases)
3. One interaction per line
4. List only the interactions, nothing else

Example output format:
riding bicycle
holding umbrella
eating pizza

Now analyze this image:</div>

        <h4>Identify a few failure cases where VLMs fail to predict the HOI classes for the given images? If so, discuss the possible reasons.</h4>

        <div class="metric-summary">
            <h4>1. The HICO-DET ground truth is poorly done</h4>
            <ul>
                <li>Doesn't capture all HOIs</li>
                <li>Sometimes just identifies an object rather than an interaction</li>
                <li>Has odd underscores "_" between verb pairs that fool analytics into thinking VLM got the category wrong</li>
            </ul>
        </div>

        <div class="metric-summary">
            <h4>2. Qwen does better on HOI metrics (where comparisons with gt can be trusted)</h4>
            <ul>
                <li>+45% better precision (16.1% vs 11.1%)</li>
                <li>+31% better F1 (0.144 vs 0.110)</li>
                <li>2 perfect scores vs Claude's 0</li>
            </ul>
        </div>

        <div class="metric-summary">
            <h4>3. Claude seems to over-predict (where comparisons with gt can be trusted)</h4>
            <ul>
                <li>Claude: 76 predictions (41% MORE than GT's 54)</li>
                <li>Qwen: 62 predictions (15% more than GT)</li>
                <li>44% "Claude-only" predictions (46 interactions) vs Qwen's 27%</li>
                <li>Classic precision/recall tradeoff: Claude appears to be recall-focused (doesn't miss anything!), Qwen is precision-focused (only say what it's sure of)</li>
            </ul>
        </div>

        <h3>Failure Cases</h3>
        <p>The three tables on the following page show the groundtruth vs the responses of the 2 VLMs. One of the most frustrating things here is that the ground truth of this dataset is not always right. E.g., image hoi_result_021.png, where one of the gt options is "umbrella." That is simply an object, not an HOI. Additionally, the gt is formatted in such a way as relationships are often parsed: "sit_on" with an underscore.</p>

        <h3>Failure Case Fixes</h3>
        <ul>
            <li>The prompt was changed to better help the VLMs understand the <em>format</em> of the gt.</li>
            <li>But with the gt so bad, it made little sense to do much else. This assisted the metrics marginally.</li>
        </ul>

        <h3>HOI Metrics and Evaluation Tables</h3>

        <div class="image-grid">
            <div class="image-container">
                <img src="results/hoi_comparison_visualizations/avg_metrics_comparison.png" alt="Average Metrics Comparison">
                <p class="image-caption">Average metrics comparison: Claude vs Qwen</p>
            </div>
            <div class="image-container">
                <img src="results/hoi_comparison_visualizations/agreement_disagreement.png" alt="VLM Agreement Analysis">
                <p class="image-caption">VLM agreement and disagreement distribution</p>
            </div>
        </div>

        <div class="image-grid">
            <div class="image-container">
                <img src="results/hoi_comparison_visualizations/per_image_precision.png" alt="Per-Image Precision">
                <p class="image-caption">Per-image precision: Claude vs Qwen</p>
            </div>
            <div class="image-container">
                <img src="results/hoi_comparison_visualizations/per_image_recall.png" alt="Per-Image Recall">
                <p class="image-caption">Per-image recall: Claude vs Qwen</p>
            </div>
        </div>

        <div class="image-grid">
            <div class="image-container">
                <img src="results/hoi_comparison_visualizations/num_interactions_comparison.png" alt="Total Interactions">
                <p class="image-caption">Total interactions across all images</p>
            </div>
            <div class="image-container">
                <img src="results/hoi_comparison_visualizations/precision_recall_scatter.png" alt="Precision-Recall Scatter">
                <p class="image-caption">Precision-recall scatter plot</p>
            </div>
        </div>

        <div class="image-container">
            <img src="results/hoi_comparison_visualizations/overall_statistics.png" alt="Summary Statistics">
            <p class="image-caption">Summary statistics and metrics</p>
        </div>

        <h3>Detailed Failure Analysis Tables</h3>
        <p>The following tables show detailed comparison between ground truth, Claude predictions, and Qwen predictions for all 30 test images:</p>

        <div class="image-container">
            <img src="results/hoi_comparison_visualizations/failure_analysis_table_part1.png" alt="Failure Analysis Table Part 1" style="max-width: 100%;">
            <p class="image-caption">Failure Analysis: Images 0-9</p>
        </div>

        <div class="image-container">
            <img src="results/hoi_comparison_visualizations/failure_analysis_table_part2.png" alt="Failure Analysis Table Part 2" style="max-width: 100%;">
            <p class="image-caption">Failure Analysis: Images 10-19</p>
        </div>

        <div class="image-container">
            <img src="results/hoi_comparison_visualizations/failure_analysis_table_part3.png" alt="Failure Analysis Table Part 3" style="max-width: 100%;">
            <p class="image-caption">Failure Analysis: Images 20-29</p>
        </div>

        <footer style="margin-top: 60px; padding-top: 20px; border-top: 2px solid #ecf0f1; text-align: center; color: #7f8c8d;">
            <p>TJ DiMeola | Computer Vision CSCI 581 | University of Mississippi</p>
            <p>Project 3: Object Detection and Human-Object Interaction Analysis | November 2025</p>
        </footer>
    </div>
</body>
</html>
